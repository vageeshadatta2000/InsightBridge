# InsightBridge: A LLM-Powered Document Analysis Tool

**[➡️ View the Live Demo Here!](https://insightbridge-llrhwpbxixnkwrdvj3gy3d.streamlit.app/)** 


InsightBridge is a demonstration project showcasing a Retrieval-Augmented Generation (RAG) pipeline for intelligent document analysis. Users can upload a PDF document and ask questions in natural language to receive context-aware, grounded answers generated by Large Language Model (LLM).


<img width="1502" alt="Demo Screenshot" src="https://github.com/user-attachments/assets/723b857e-79e7-4d31-b459-928ffa73136a" />









> **Project Update: Migration from OpenAI to Google Gemini**
> This project was originally built using the OpenAI API. To ensure a sustainable and cost-effective public demo for my portfolio, I successfully migrated the entire pipeline to use the **Google Gemini API**. This change demonstrates adaptability and the ability to work with different major LLM providers.

## Technologies Used

*   **Language:** Python
*   **LLM Framework:** LangChain
*   **LLM & Embeddings:** Google Gemini (`gemini-1.5-flash-latest`)
*   **Vector Store:** FAISS (Facebook AI Similarity Search) - CPU Version
*   **UI Framework:** Streamlit
*   **PDF Processing:** PyPDF

## Core Features & Technical Implementation

This project directly implements the following core concepts:

1.  **Recursive Text Chunking Pipeline:**
    *   To handle long-form documents efficiently, I designed and implemented a text chunking pipeline using LangChain’s `RecursiveCharacterTextSplitter`.
    *   This splitter intelligently divides the document into smaller, semantically meaningful chunks, which is crucial for effective vectorization and retrieval.

2.  **Low-Latency Vector Store and Retrieval:**
    *   I integrated a **FAISS (Facebook AI Similarity Search)**-based vector store for fast, in-memory similarity searches.
    *   Text chunks are converted into high-dimensional vectors (embeddings) using **Google's `embedding-001` model** (via `langchain-google-genai`) and stored in the FAISS index. This enables low-latency dense retrieval, allowing the system to quickly find the most relevant document chunks for a given user query.

3.  **Retrieval-Augmented Generation (RAG) Chain:**
    *   I constructed a RAG chain using LangChain, which connects the FAISS retriever to **Google's `gemini-1.5-flash-latest` model** (via the `ChatGoogleGenerativeAI` class).
    *   When a user asks a question, the system first retrieves the most relevant text chunks from the document (the "context"). This context is then passed to the Gemini model along with the original question, instructing it to generate a grounded, context-aware response based *only* on the provided information. This significantly reduces hallucinations and improves the factual accuracy of the answers.

## How to Run the Project Locally

Follow these steps to set up and run InsightBridge on your local machine.

### Prerequisites

*   Python 3.11+
*   A **Google API Key** (obtainable from [Google AI Studio](https://aistudio.google.com/app/apikey))

### Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/insightbridge.git
    cd insightbridge
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    # For Mac/Linux
    python3 -m venv venv
    source venv/bin/activate

    # For Windows
    python -m venv venv
    .\venv\Scripts\activate
    ```

3.  **Install the system dependencies (for Faiss):**
    *   On Debian/Ubuntu: `sudo apt-get install swig libomp-dev build-essential`
    *   On MacOS: `brew install swig libomp`

4.  **Install the required Python packages:**
    ```bash
    pip install -r requirements.txt
    ```

### Running the Application

1.  **Set up your API Key:** Create a file named `.env` in the root of the project and add your Google API key to it:
    ```
    GOOGLE_API_KEY="AIzaSy...xxxxxxxxxx"
    ```

2.  **Adapt `app.py` for Local Use:** The deployed code is configured to read secrets from Streamlit Cloud. For local development, you need to use a library to read your `.env` file.
    
    *   **Find this section** in `app.py`:
        ```python
        # --- MODIFICATION: API Key Management for Google Gemini ---
        try:
            google_api_key = st.secrets["GOOGLE_API_KEY"]
            genai.configure(api_key=google_api_key) 
        except KeyError:
            st.error("Google API key not found. Please add it to your Streamlit secrets.")
            st.info("For more info, see https://docs.streamlit.io/deploy/concepts/secrets-management")
            st.stop()
        # --- END MODIFICATION ---
        ```
    *   **Replace it with this** for local testing:
        ```python
        # --- MODIFICATION: API Key Management for Google Gemini ---
        from dotenv import load_dotenv
        load_dotenv()
        
        try:
            google_api_key = os.getenv("GOOGLE_API_KEY")
            genai.configure(api_key=google_api_key)
        except ValueError:
            st.error("Google API key not found. Please add it to your .env file.")
            st.stop()
        # --- END MODIFICATION ---
        ```

3.  **Start the Streamlit app:**
    ```bash
    streamlit run app.py
    ```
